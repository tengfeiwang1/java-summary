
# ZOOKEEPER
## 简介

   ZooKeeper 是一个典型的分布式**数据一致性**解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如*数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列*等功能。
## 应用场景
   1. Zookeeper 一个最常用的使用场景就是用于担任服务生产者和服务消费者的**注册中心**,在CAP理论上，对A做了让步和妥协（多节 点保证可用性）
   
## 重要概念
1. ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。
为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。
2. ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持znode中存储的数据量较小的进一步原因）。
ZooKeeper 是高性能的(但不是高可用的:网络抖动，宕机等)。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。）
3. ZooKeeper有临时节点的概念。 *当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个ZNode被创建了，除非主动进行ZNode的移除操作，否则这个ZNode将一直保存在Zookeeper上。*
4. ZooKeeper 底层其实只提供了两个功能：1 管理（存储、读取）用户程序提交的数据；2 为用户程序提供数据节点监听服务。

## 特点
1. 顺序一致性： 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。
2. 原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。
3. 单一系统映像 ： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。
4. 可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。

>> Zookeeper一致性原理：（ZAB协议(ZooKeeper Atomic Broadcast)）http://blog.51cto.com/welcomeweb/2103292?utm_source=oschina-app
   Zab协议有两种模式， **崩溃恢复**（选主+数据同步）和**消息广播**（事务操作）。任何时候都需要保证只有一个主进程负责进行事务操作，而如果主进程崩溃了，就需要迅速选举出一个新的主进程。主进程的选举机制与事务操作机制是紧密相关的。下面详细讲解这三个场景的协议规则，从细节去探索ZAB协议的数据一致性原理。

这不仅得益于zk类文件系统的数据模型和基于Watcher机制的分布式事件通知，也得益于zk特殊的高容错数据一致性协议。

## 缺点
1. 不适合大量写，会造成io过高问题；
2. 存储数据过大时，容易在同步数据时超时等
3. 不是高可用的
4. 无法进行有效的权限控制
5. 性能有限

## 选举算法：
  **基于TCP的fast leader election： http://www.jasongj.com/zookeeper/fastleaderelection/**
  FastLeaderElection选举算法是标准的Fast Paxos算法实现，可解决LeaderElection选举算法收敛速度慢的问题。
- 总结
   1. 由于使用主从复制模式，所有的写操作都要由Leader主导完成，而读操作可通过任意节点完成，因此Zookeeper读性能远好于写性能，更适合读多写少的场景
   2. 虽然使用主从复制模式，同一时间只有一个Leader，但是Failover机制保证了集群不存在单点失败（SPOF）的问题
   3. ZAB协议保证了Failover过程中的数据一致性
   4. 服务器收到数据后先写本地文件再进行处理，保证了数据的持久性

[原理解析](https://www.cnblogs.com/qingyunzong/p/8632995.html)

[paxos raft](https://www.cnblogs.com/zhang-qc/p/8688258.html)

[选举机制](https://segmentfault.com/a/1190000039385874)
## api应用
https://www.cnblogs.com/qingyunzong/p/8666288.html
